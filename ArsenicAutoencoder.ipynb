{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5FqDR3aeYFG9xCQhS/YT1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushirito/ArsenicFR/blob/main/ArsenicAutoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Colab setup ===\n",
        "# If in Colab, run this to mount Drive\n",
        "from google.colab import drive  # safe if not in Colab\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyMX55NLGTu9",
        "outputId": "cabc70b5-d169-4e8d-ef44-1dc16addf3ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUlWnBMwFP3r",
        "outputId": "2a67e61c-2944-4199-9891-6036a8e1898f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wavelengths: (601,) Concentrations: [ 0. 10. 20. 30. 40. 60.] A: (601, 6)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Optional. Colab already has torch. Keep this if you want a fixed version.\n",
        "# !pip -q install torch torchvision torchaudio\n",
        "\n",
        "import os, math, time, random, pickle, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility and dtype\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"medium\")\n",
        "except Exception:\n",
        "    pass\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "ROOT = \"/content/drive/MyDrive/ArsenicSTS\"\n",
        "DATA_CSV = f\"{ROOT}/UVVisData/0.30MB_AuNP_As.csv\"\n",
        "CKPT_ROOT = f\"{ROOT}/aae_ckpts\"\n",
        "FIG_ROOT = f\"{ROOT}/figs\"\n",
        "GEN_ROOT = f\"{ROOT}/generated\"\n",
        "os.makedirs(CKPT_ROOT, exist_ok=True)\n",
        "os.makedirs(FIG_ROOT, exist_ok=True)\n",
        "os.makedirs(GEN_ROOT, exist_ok=True)\n",
        "\n",
        "# ===== Data loader and preprocessing =====\n",
        "def load_uvvis_csv(path, baseline_correct=True):\n",
        "    \"\"\"Returns wavelengths [601], conc_list [6], A_mat [601,6] after optional baseline removal at 800 nm.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    # normalize column names to str\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    assert \"Wavelength\" in df.columns, \"CSV must have 'Wavelength' column\"\n",
        "    wl = df[\"Wavelength\"].to_numpy().astype(np.float32)\n",
        "    # extract concentration columns that are numeric when cast to float\n",
        "    conc_cols = []\n",
        "    for c in df.columns:\n",
        "        if c == \"Wavelength\":\n",
        "            continue\n",
        "        try:\n",
        "            float(c)\n",
        "            conc_cols.append(c)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # sort by numeric concentration ascending\n",
        "    conc_vals = np.array(sorted([float(c) for c in conc_cols], key=float), dtype=np.float32)\n",
        "    conc_cols_sorted = [str(int(c)) if float(c).is_integer() else str(c) for c in conc_vals]\n",
        "    # Build matrix in that order\n",
        "    A = np.stack([df[c].to_numpy().astype(np.float32) for c in conc_cols_sorted], axis=1)  # [601,6]\n",
        "    # Optional baseline correction using A at 800 nm\n",
        "    if baseline_correct:\n",
        "        # find index of 800 nm\n",
        "        idx_800 = int(np.argmin(np.abs(wl - 800.0)))\n",
        "        base = A[idx_800:idx_800+1, :]  # [1,6]\n",
        "        A = A - base\n",
        "    return wl, conc_vals, A\n",
        "\n",
        "def split_loco(concs):\n",
        "    \"\"\"Yield folds with one held concentration each.\"\"\"\n",
        "    for c_hold in concs:\n",
        "        train = [c for c in concs if c != c_hold]\n",
        "        yield c_hold, np.array(train, dtype=np.float32)\n",
        "\n",
        "class SpectraScaler:\n",
        "    \"\"\"Featurewise standardization using training stats only. Works on [N,601] row vectors.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "    def fit(self, X):  # X [N,601]\n",
        "        self.mean = X.mean(axis=0, keepdims=True)\n",
        "        self.std  = X.std(axis=0, keepdims=True) + 1e-6\n",
        "    def transform(self, X):\n",
        "        return (X - self.mean) / self.std\n",
        "    def inverse(self, Xz):\n",
        "        return Xz * self.std + self.mean\n",
        "    def save(self, path):\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump({\"mean\": self.mean, \"std\": self.std}, f)\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            d = pickle.load(f)\n",
        "        sc = SpectraScaler()\n",
        "        sc.mean, sc.std = d[\"mean\"], d[\"std\"]\n",
        "        return sc\n",
        "\n",
        "# Load data\n",
        "wl, concs_all, A = load_uvvis_csv(DATA_CSV, baseline_correct=True)  # A [601,6]\n",
        "assert A.shape[0] == 601, \"Expected 601 wavelengths\"\n",
        "assert A.shape[1] == len(concs_all), \"Mismatch in concentration columns\"\n",
        "print(\"Wavelengths:\", wl.shape, \"Concentrations:\", concs_all, \"A:\", A.shape)\n",
        "\n",
        "# Build per-spectrum dataset\n",
        "# Each column is a sample x in R^601 and scalar c in µg/L\n",
        "SPECTRA = {float(c): A[:, i].astype(np.float32) for i, c in enumerate(concs_all)}\n",
        "WL_MIN, WL_MAX = wl.min(), wl.max()\n",
        "C_MIN, C_MAX = float(concs_all.min()), float(concs_all.max())\n",
        "\n",
        "def scale_c(c, cmin=C_MIN, cmax=C_MAX):\n",
        "    return (c - cmin) / (cmax - cmin + 1e-12)\n",
        "\n",
        "def unscale_c(cs, cmin=C_MIN, cmax=C_MAX):\n",
        "    return cs * (cmax - cmin) + cmin\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Small MLP building block =====\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, out_dim, n_layers=3, act=nn.SiLU, layernorm=True, final_act=None):\n",
        "        super().__init__()\n",
        "        dims = [in_dim] + [hidden]*n_layers + [out_dim]\n",
        "        layers = []\n",
        "        for i in range(len(dims)-2):\n",
        "            layers += [nn.Linear(dims[i], dims[i+1])]\n",
        "            if layernorm:\n",
        "                layers += [nn.LayerNorm(dims[i+1])]\n",
        "            layers += [act()]\n",
        "        layers += [nn.Linear(dims[-2], dims[-1])]\n",
        "        if final_act is not None:\n",
        "            layers += [final_act()]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ===== AAE components =====\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim=601, hidden=256, latent_dim=4, n_layers=3):\n",
        "        super().__init__()\n",
        "        self.f = MLP(in_dim, hidden, latent_dim, n_layers=n_layers, act=nn.SiLU, layernorm=True)\n",
        "    def forward(self, x):\n",
        "        return self.f(x)\n",
        "\n",
        "class CEmbed(nn.Module):\n",
        "    def __init__(self, emb_dim=8):\n",
        "        super().__init__()\n",
        "        self.f = MLP(1, hidden=16, out_dim=emb_dim, n_layers=1, act=nn.SiLU, layernorm=False)\n",
        "    def forward(self, c01):\n",
        "        return self.f(c01)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=4, c_emb_dim=8, hidden=256, out_dim=601, n_layers=3):\n",
        "        super().__init__()\n",
        "        self.f = MLP(latent_dim + c_emb_dim, hidden, out_dim, n_layers=n_layers, act=nn.SiLU, layernorm=True)\n",
        "    def forward(self, z, cemb):\n",
        "        h = torch.cat([z, cemb], dim=-1)\n",
        "        return self.f(h)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, latent_dim=4, hidden=128):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.f(z)  # logits\n",
        "\n",
        "class AuxRegressor(nn.Module):\n",
        "    def __init__(self, latent_dim=4):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32), nn.SiLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.f(z)  # predicts c_scaled\n"
      ],
      "metadata": {
        "id": "IC8Vb_wfGi4V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Utilities =====\n",
        "def rmse(a, b):\n",
        "    return float(np.sqrt(np.mean((a - b)**2)))\n",
        "\n",
        "def pearsonr(a, b):\n",
        "    a = a - a.mean(); b = b - b.mean()\n",
        "    return float((a*b).sum() / (np.sqrt((a*a).sum()) * np.sqrt((b*b).sum()) + 1e-12))\n",
        "\n",
        "def peak_nm(wl, spec):\n",
        "    return float(wl[int(np.argmax(spec))])\n",
        "\n",
        "def linear_baseline_interp(concs_known, A_known, c_target):\n",
        "    \"\"\"Interpolate at each wavelength separately using two nearest neighbors in conc space.\n",
        "       A_known shape [601, K] for K known concs in ascending concs_known.\"\"\"\n",
        "    concs = np.array(concs_known, dtype=np.float32)\n",
        "    A_kn = np.array(A_known, dtype=np.float32)  # [601,K]\n",
        "    # find left and right neighbors for c_target\n",
        "    if c_target <= concs.min():\n",
        "        # extrapolate using first two\n",
        "        i0, i1 = 0, 1\n",
        "    elif c_target >= concs.max():\n",
        "        i0, i1 = len(concs)-2, len(concs)-1\n",
        "    else:\n",
        "        i1 = int(np.searchsorted(concs, c_target, side=\"right\"))\n",
        "        i0 = i1 - 1\n",
        "    c0, c1 = concs[i0], concs[i1]\n",
        "    w = (c_target - c0) / (c1 - c0 + 1e-12)\n",
        "    return (1 - w) * A_kn[:, i0] + w * A_kn[:, i1]\n",
        "\n",
        "def make_batch(X, C, batch_size):\n",
        "    n = X.shape[0]\n",
        "    idx = np.random.randint(0, n, size=(batch_size,))\n",
        "    return torch.from_numpy(X[idx]).float().to(DEVICE), torch.from_numpy(C[idx]).float().to(DEVICE)\n",
        "\n",
        "def save_fold_artifacts(fold_dir, enc, dec, disc, aux, scaler, meta):\n",
        "    os.makedirs(fold_dir, exist_ok=True)\n",
        "    torch.save(enc.state_dict(),  os.path.join(fold_dir, \"encoder.pt\"))\n",
        "    torch.save(dec.state_dict(),  os.path.join(fold_dir, \"decoder.pt\"))\n",
        "    torch.save(disc.state_dict(), os.path.join(fold_dir, \"disc.pt\"))\n",
        "    torch.save(aux.state_dict(),  os.path.join(fold_dir, \"aux.pt\"))\n",
        "    scaler.save(os.path.join(fold_dir, \"scaler.pkl\"))\n",
        "    with open(os.path.join(fold_dir, \"meta.json\"), \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "def load_fold(fold_dir, latent_dim=4, c_emb_dim=8):\n",
        "    enc = Encoder(latent_dim=latent_dim).to(DEVICE)\n",
        "    dec = Decoder(latent_dim=latent_dim, c_emb_dim=c_emb_dim).to(DEVICE)\n",
        "    disc = Discriminator(latent_dim=latent_dim).to(DEVICE)\n",
        "    aux = AuxRegressor(latent_dim=latent_dim).to(DEVICE)\n",
        "    enc.load_state_dict(torch.load(os.path.join(fold_dir, \"encoder.pt\"), map_location=DEVICE))\n",
        "    dec.load_state_dict(torch.load(os.path.join(fold_dir, \"decoder.pt\"), map_location=DEVICE))\n",
        "    disc.load_state_dict(torch.load(os.path.join(fold_dir, \"disc.pt\"), map_location=DEVICE))\n",
        "    aux.load_state_dict(torch.load(os.path.join(fold_dir, \"aux.pt\"), map_location=DEVICE))\n",
        "    scaler = SpectraScaler.load(os.path.join(fold_dir, \"scaler.pkl\"))\n",
        "    with open(os.path.join(fold_dir, \"meta.json\"), \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "    cembed = CEmbed(emb_dim=meta[\"c_emb_dim\"]).to(DEVICE)\n",
        "    cembed.load_state_dict(torch.load(os.path.join(fold_dir, \"cembed.pt\"), map_location=DEVICE))\n",
        "    return enc.eval(), dec.eval(), disc.eval(), aux.eval(), cembed.eval(), scaler, meta\n"
      ],
      "metadata": {
        "id": "GF6jOcW0GjZX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Core trainer for one LOCO fold =====\n",
        "def train_fold(fold_spec,\n",
        "               latent_dim=4, c_emb_dim=8, batch_size=32,\n",
        "               max_steps=6000, patience=500,\n",
        "               w_rec=0.9, w_adv=0.1, w_aux=0.05,\n",
        "               lr=4e-4, lr_min=1e-5):\n",
        "\n",
        "    c_hold = float(fold_spec[\"c_hold\"])\n",
        "    train_concs = np.array(fold_spec[\"train_concs\"], dtype=np.float32)\n",
        "    fold_dir = os.path.join(CKPT_ROOT, f\"fold_{int(c_hold)}\")\n",
        "    os.makedirs(fold_dir, exist_ok=True)\n",
        "\n",
        "    # Build training arrays\n",
        "    X_train = np.stack([SPECTRA[c] for c in train_concs], axis=0)  # [5,601]\n",
        "    C_train = np.array([scale_c(c) for c in train_concs], dtype=np.float32).reshape(-1,1)\n",
        "\n",
        "    scaler = SpectraScaler(); scaler.fit(X_train)  # featurewise\n",
        "    Xz = scaler.transform(X_train).astype(np.float32)\n",
        "\n",
        "    # Tiny dataset. We will repeatedly sample minibatches from it.\n",
        "    # Validation proxy will be EMA of training recon loss.\n",
        "    # Models\n",
        "    enc = Encoder(latent_dim=latent_dim).to(DEVICE)\n",
        "    cembed = CEmbed(emb_dim=c_emb_dim).to(DEVICE)\n",
        "    dec = Decoder(latent_dim=latent_dim, c_emb_dim=c_emb_dim).to(DEVICE)\n",
        "    disc = Discriminator(latent_dim=latent_dim).to(DEVICE)\n",
        "    aux = AuxRegressor(latent_dim=latent_dim).to(DEVICE)\n",
        "\n",
        "    # Opts and sched\n",
        "    opt_g = torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()) +\n",
        "                             list(cembed.parameters()) + list(aux.parameters()), lr=lr)\n",
        "    T_max = max_steps\n",
        "    sch_g = torch.optim.lr_scheduler.CosineAnnealingLR(opt_g, T_max=T_max, eta_min=lr_min)\n",
        "\n",
        "    opt_d = torch.optim.Adam(disc.parameters(), lr=lr*0.5)\n",
        "\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "    best_rec = float(\"inf\")\n",
        "    best_step = 0\n",
        "    ema_rec = None\n",
        "    hist = {\"step\": [], \"rec\": [], \"adv\": [], \"aux\": []}\n",
        "\n",
        "    Xnp = Xz.astype(np.float32); Cnp = C_train.astype(np.float32)\n",
        "\n",
        "    for step in range(1, max_steps+1):\n",
        "        # Make a batch by resampling from the 5 rows\n",
        "        xb, cb = make_batch(Xnp, Cnp, batch_size)\n",
        "        z = enc(xb)  # [B,L]\n",
        "        cemb = cembed(cb)  # [B,E]\n",
        "        x_rec = dec(z, cemb)\n",
        "\n",
        "        # --- Discriminator update ---\n",
        "        # Prior samples as \"real\"\n",
        "        z_real = torch.randn_like(z)\n",
        "        logits_real = disc(z_real)\n",
        "        logits_fake = disc(z.detach())\n",
        "        d_loss = bce(logits_real, torch.ones_like(logits_real)) + \\\n",
        "                 bce(logits_fake, torch.zeros_like(logits_fake))\n",
        "        opt_d.zero_grad(set_to_none=True)\n",
        "        d_loss.backward()\n",
        "        opt_d.step()\n",
        "\n",
        "        # --- Generator (E+D+H) update ---\n",
        "        rec = mse(x_rec, xb)\n",
        "        # fool the discriminator\n",
        "        logits_fake2 = disc(z)\n",
        "        adv = bce(logits_fake2, torch.ones_like(logits_fake2))\n",
        "        # auxiliary\n",
        "        c_pred = aux(z)\n",
        "        aux_loss = mse(c_pred, cb)\n",
        "        loss = w_rec*rec + w_adv*adv + w_aux*aux_loss\n",
        "\n",
        "        opt_g.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt_g.step()\n",
        "        sch_g.step()\n",
        "\n",
        "        # Track\n",
        "        rec_val = float(rec.detach().cpu())\n",
        "        ema_rec = rec_val if ema_rec is None else 0.98*ema_rec + 0.02*rec_val\n",
        "        hist[\"step\"].append(step); hist[\"rec\"].append(rec_val)\n",
        "        hist[\"adv\"].append(float(adv.detach().cpu()))\n",
        "        hist[\"aux\"].append(float(aux_loss.detach().cpu()))\n",
        "\n",
        "        # Early stopping on best raw rec\n",
        "        if rec_val < best_rec - 1e-6:\n",
        "            best_rec = rec_val\n",
        "            best_step = step\n",
        "            # Save best so far\n",
        "            torch.save(enc.state_dict(),  os.path.join(fold_dir, \"encoder.pt\"))\n",
        "            torch.save(dec.state_dict(),  os.path.join(fold_dir, \"decoder.pt\"))\n",
        "            torch.save(disc.state_dict(), os.path.join(fold_dir, \"disc.pt\"))\n",
        "            torch.save(aux.state_dict(),  os.path.join(fold_dir, \"aux.pt\"))\n",
        "            torch.save(cembed.state_dict(), os.path.join(fold_dir, \"cembed.pt\"))\n",
        "            scaler.save(os.path.join(fold_dir, \"scaler.pkl\"))\n",
        "            with open(os.path.join(fold_dir, \"train_hist.json\"), \"w\") as f:\n",
        "                json.dump(hist, f)\n",
        "        if step - best_step >= patience:\n",
        "            print(f\"[fold {int(c_hold)}] Early stop at {step}. Best step {best_step} rec {best_rec:.4e}\")\n",
        "            break\n",
        "\n",
        "    # Meta\n",
        "    meta = {\n",
        "        \"c_hold\": float(c_hold),\n",
        "        \"train_concs\": [float(x) for x in train_concs.tolist()],\n",
        "        \"latent_dim\": int(latent_dim),\n",
        "        \"c_emb_dim\": int(c_emb_dim),\n",
        "        \"best_step\": int(best_step),\n",
        "        \"best_rec\": float(best_rec),\n",
        "        \"w_rec\": float(w_rec), \"w_adv\": float(w_adv), \"w_aux\": float(w_aux),\n",
        "        \"lr\": float(lr), \"lr_min\": float(lr_min),\n",
        "        \"max_steps\": int(max_steps), \"patience\": int(patience)\n",
        "    }\n",
        "    with open(os.path.join(fold_dir, \"meta.json\"), \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    # Return quick info\n",
        "    return {\"fold_dir\": fold_dir, \"meta\": meta, \"best_rec\": best_rec}\n"
      ],
      "metadata": {
        "id": "VsS-wRKyG2ES"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Generation helpers =====\n",
        "def decode_many(dec, cembed, scaler, c_target, N, latent_dim, c_emb_dim):\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(N, latent_dim, device=DEVICE)\n",
        "        c01 = torch.full((N,1), fill_value=scale_c(c_target), device=DEVICE)\n",
        "        cemb = cembed(c01)\n",
        "        xz = dec(z, cemb)  # standardized\n",
        "        x = scaler.inverse(xz.detach().cpu().numpy())\n",
        "    return x  # [N,601]\n",
        "\n",
        "def filtered_reject(dec, cembed, aux, scaler, c_target, N, latent_dim, eps=0.02, max_trials=20000):\n",
        "    out = []\n",
        "    trials = 0\n",
        "    ct = torch.tensor([[scale_c(c_target)]], device=DEVICE)\n",
        "    while len(out) < N and trials < max_trials:\n",
        "        trials += 256\n",
        "        z = torch.randn(256, latent_dim, device=DEVICE)\n",
        "        c_pred = aux(z)\n",
        "        mask = (c_pred - ct).abs() <= eps\n",
        "        if mask.any():\n",
        "            z_keep = z[mask.squeeze(1)]\n",
        "            with torch.no_grad():\n",
        "                cemb = cembed(ct.repeat(z_keep.size(0),1))\n",
        "                xz = dec(z_keep, cemb)\n",
        "            x = scaler.inverse(xz.detach().cpu().numpy())\n",
        "            out.append(x)\n",
        "    if len(out) == 0:\n",
        "        return np.zeros((0, 601), dtype=np.float32)\n",
        "    X = np.concatenate(out, axis=0)\n",
        "    return X[:N]\n",
        "\n",
        "def filtered_opt(dec, cembed, aux, scaler, c_target, N, latent_dim, steps=200, lr=0.05, lam=0.01, sigma=0.05):\n",
        "    # gradient-based targeting toward c*\n",
        "    X = []\n",
        "    ct = torch.tensor([[scale_c(c_target)]], device=DEVICE)\n",
        "    for _ in range(N):\n",
        "        z = torch.randn(1, latent_dim, device=DEVICE, requires_grad=True)\n",
        "        opt = torch.optim.Adam([z], lr=lr)\n",
        "        for _ in range(steps):\n",
        "            cp = aux(z)\n",
        "            loss = (cp - ct).pow(2).mean() + lam*(z.pow(2).mean())\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        # small local noise\n",
        "        z_final = z.detach() + sigma*torch.randn_like(z)\n",
        "        with torch.no_grad():\n",
        "            cemb = cembed(ct)\n",
        "            xz = dec(z_final, cemb)\n",
        "        X.append(scaler.inverse(xz.detach().cpu().numpy()))\n",
        "    return np.concatenate(X, axis=0)\n",
        "\n",
        "# ===== Public API =====\n",
        "_LAST_FOLD_CACHE = {}  # c_hold -> loaded modules\n",
        "\n",
        "def _pick_fold_for_c(c_target):\n",
        "    # Choose fold that held the nearest concentration to c_target\n",
        "    diffs = {float(c): abs(c_target - float(c)) for c in concs_all}\n",
        "    nearest = min(diffs, key=diffs.get)\n",
        "    return int(nearest)\n",
        "\n",
        "def _load_fold_cached(c_hold):\n",
        "    if c_hold in _LAST_FOLD_CACHE:\n",
        "        return _LAST_FOLD_CACHE[c_hold]\n",
        "    fold_dir = os.path.join(CKPT_ROOT, f\"fold_{int(c_hold)}\")\n",
        "    enc, dec, disc, aux, cemb, scaler, meta = load_fold(fold_dir)\n",
        "    _LAST_FOLD_CACHE[c_hold] = (enc, dec, disc, aux, cemb, scaler, meta)\n",
        "    return _LAST_FOLD_CACHE[c_hold]\n",
        "\n",
        "def generate_spectra(c_target, N=64, mode=\"conditional\"):\n",
        "    \"\"\"Returns np.ndarray [N,601] in baseline-corrected absorbance space.\"\"\"\n",
        "    fold_to_use = _pick_fold_for_c(c_target)\n",
        "    enc, dec, disc, aux, cemb, scaler, meta = _load_fold_cached(fold_to_use)\n",
        "    latent_dim = meta[\"latent_dim\"]; c_emb_dim = meta[\"c_emb_dim\"]\n",
        "\n",
        "    if mode == \"conditional\":\n",
        "        X = decode_many(dec, cemb, scaler, c_target, N, latent_dim, c_emb_dim)\n",
        "    elif mode == \"filtered_reject\":\n",
        "        X = filtered_reject(dec, cemb, aux, scaler, c_target, N, latent_dim, eps=0.02)\n",
        "    elif mode == \"filtered_opt\":\n",
        "        X = filtered_opt(dec, cemb, aux, scaler, c_target, N, latent_dim, steps=200, lr=0.05, lam=0.01, sigma=0.05)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be one of {'conditional','filtered_reject','filtered_opt'}\")\n",
        "    # Save optional dump\n",
        "    np.save(os.path.join(GEN_ROOT, f\"gen_c_{int(round(c_target))}_{mode}.npy\"), X.astype(np.float32))\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "tdFeifgeGm3y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loco(N=64, modes=(\"conditional\",\"filtered_opt\")):\n",
        "    rows = []\n",
        "    for c_hold, train_concs in split_loco(concs_all):\n",
        "        # train this fold if not already present\n",
        "        fold_dir = os.path.join(CKPT_ROOT, f\"fold_{int(c_hold)}\")\n",
        "        if not os.path.exists(os.path.join(fold_dir, \"encoder.pt\")):\n",
        "            print(f\"Training fold for held {int(c_hold)}\")\n",
        "            train_fold({\"c_hold\": float(c_hold), \"train_concs\": train_concs})\n",
        "        # load trained\n",
        "        enc, dec, disc, aux, cemb, scaler, meta = load_fold(fold_dir)\n",
        "        latent_dim = meta[\"latent_dim\"]; c_emb_dim = meta[\"c_emb_dim\"]\n",
        "\n",
        "        # Real held spectrum in baseline-corrected space with same scaler\n",
        "        x_true = SPECTRA[float(c_hold)]\n",
        "        # for baseline comparison we build linear interpolation from training concs and spectra\n",
        "        A_train_mat = np.stack([SPECTRA[float(c)] for c in train_concs], axis=1)\n",
        "        lin_pred = linear_baseline_interp(train_concs, A_train_mat, float(c_hold))\n",
        "\n",
        "        # For metrics, we compare mean generated spectrum to real held\n",
        "        for mode in modes:\n",
        "            if mode == \"conditional\":\n",
        "                X = decode_many(dec, cemb, scaler, float(c_hold), N, latent_dim, c_emb_dim)\n",
        "            elif mode == \"filtered_opt\":\n",
        "                X = filtered_opt(dec, cemb, aux, scaler, float(c_hold), N, latent_dim, steps=200, lr=0.05, lam=0.01, sigma=0.05)\n",
        "            elif mode == \"filtered_reject\":\n",
        "                X = filtered_reject(dec, cemb, aux, scaler, float(c_hold), N, latent_dim, eps=0.02)\n",
        "            else:\n",
        "                continue\n",
        "            x_mean = X.mean(axis=0)\n",
        "\n",
        "            r = pearsonr(x_true, x_mean)\n",
        "            e = rmse(x_true, x_mean)\n",
        "            p_err = abs(peak_nm(wl, x_true) - peak_nm(wl, x_mean))\n",
        "\n",
        "            # linear baseline metrics\n",
        "            e_lin = rmse(x_true, lin_pred)\n",
        "            r_lin = pearsonr(x_true, lin_pred)\n",
        "\n",
        "            rows.append({\n",
        "                \"c_hold\": float(c_hold),\n",
        "                \"mode\": mode,\n",
        "                \"rmse\": e,\n",
        "                \"pearson_r\": r,\n",
        "                \"peak_shift_nm\": p_err,\n",
        "                \"rmse_linear\": e_lin,\n",
        "                \"pearson_r_linear\": r_lin\n",
        "            })\n",
        "\n",
        "            # Plot overlay and uncertainty band\n",
        "            fig = plt.figure(figsize=(8,4))\n",
        "            plt.plot(wl, x_true, label=f\"Real c={int(c_hold)}\")\n",
        "            plt.plot(wl, x_mean, label=f\"AAE mean {mode}\")\n",
        "            # uncertainty band\n",
        "            std = X.std(axis=0)\n",
        "            plt.fill_between(wl, x_mean-std, x_mean+std, alpha=0.2, label=\"AAE ±1 sd\")\n",
        "            # linear baseline\n",
        "            plt.plot(wl, lin_pred, linestyle=\":\", label=\"Linear baseline\")\n",
        "            plt.xlabel(\"Wavelength (nm)\"); plt.ylabel(\"Absorbance (baseline corrected)\")\n",
        "            plt.legend(loc=\"best\")\n",
        "            plt.tight_layout()\n",
        "            outpng = os.path.join(FIG_ROOT, f\"loco_c_{int(c_hold)}_{mode}.png\")\n",
        "            plt.savefig(outpng, dpi=160); plt.close(fig)\n",
        "\n",
        "        # Save a CSV of generated mean spectra for this held concentration\n",
        "        out_csv = os.path.join(GEN_ROOT, f\"loco_c_{int(c_hold)}.csv\")\n",
        "        cols = {}\n",
        "        for mode in modes:\n",
        "            # regenerate to avoid storing large arrays\n",
        "            if mode == \"conditional\":\n",
        "                X = decode_many(dec, cemb, scaler, float(c_hold), N, latent_dim, c_emb_dim)\n",
        "            else:\n",
        "                X = filtered_opt(dec, cemb, aux, scaler, float(c_hold), N, latent_dim)\n",
        "            cols[f\"gen_mean_{mode}\"] = X.mean(axis=0)\n",
        "        df = pd.DataFrame({\"Wavelength\": wl, \"Real\": x_true, \"Linear\": lin_pred, **cols})\n",
        "        df.to_csv(out_csv, index=False)\n",
        "\n",
        "    dfres = pd.DataFrame(rows)\n",
        "    dfres_path = os.path.join(ROOT, \"loco_results.csv\")\n",
        "    dfres.to_csv(dfres_path, index=False)\n",
        "    print(\"Saved LOCO results to\", dfres_path)\n",
        "    return dfres\n",
        "\n",
        "def generate_grid(c_values, N_per=32, mode=\"conditional\"):\n",
        "    out = {}\n",
        "    rows = []\n",
        "    for c in c_values:\n",
        "        X = generate_spectra(c, N=N_per, mode=mode)\n",
        "        out[c] = X\n",
        "        rows.append(np.concatenate([[c], X.mean(axis=0)], axis=0))\n",
        "    grid = np.stack(rows, axis=0)\n",
        "    cols = [\"c\"] + [f\"{int(w)}\" for w in wl]\n",
        "    df = pd.DataFrame(grid, columns=cols)\n",
        "    csv_path = os.path.join(GEN_ROOT, \"gen_grid_interp.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "1NObpzyPG4Tp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Train all folds and print LOCO table =====\n",
        "metrics = evaluate_loco()   # trains missing folds, evaluates modes: conditional and filtered_opt\n",
        "print(metrics)\n",
        "\n",
        "# ===== Interpolate at c*=25 using conditional decoding =====\n",
        "specs = generate_spectra(25.0, N=64, mode=\"conditional\")\n",
        "print(\"Generated specs shape:\", specs.shape)\n",
        "\n",
        "# ===== Same using filtered optimization =====\n",
        "specs_opt = generate_spectra(25.0, N=64, mode=\"filtered_opt\")\n",
        "print(\"Generated specs_opt shape:\", specs_opt.shape)\n",
        "\n",
        "# ===== Smoothness diagnostic across midpoints =====\n",
        "midpoints = [5, 15, 25, 35, 50]\n",
        "grid = generate_grid(midpoints, N_per=32, mode=\"conditional\")\n",
        "\n",
        "# Simple smoothness score by total variation across concentrations\n",
        "means = np.stack([grid[c].mean(axis=0) for c in midpoints], axis=0)  # [M,601]\n",
        "tv = np.abs(np.diff(means, axis=0)).sum()  # scalar\n",
        "print(\"Total variation across c midpoints:\", float(tv))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCkipUX7G6h9",
        "outputId": "f27c9d9d-aaf7-408a-b7a1-92dba5ac3410"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold for held 0\n",
            "[fold 0] Early stop at 1983. Best step 1483 rec 2.2747e-05\n",
            "Training fold for held 10\n",
            "[fold 10] Early stop at 1395. Best step 895 rec 3.2301e-05\n",
            "Training fold for held 20\n",
            "[fold 20] Early stop at 1006. Best step 506 rec 1.7088e-05\n",
            "Training fold for held 30\n",
            "[fold 30] Early stop at 1117. Best step 617 rec 1.6933e-05\n",
            "Training fold for held 40\n",
            "[fold 40] Early stop at 1725. Best step 1225 rec 2.4530e-05\n",
            "Training fold for held 60\n",
            "[fold 60] Early stop at 1378. Best step 878 rec 6.8582e-05\n",
            "Saved LOCO results to /content/drive/MyDrive/ArsenicSTS/loco_results.csv\n",
            "    c_hold          mode      rmse  pearson_r  peak_shift_nm  rmse_linear  \\\n",
            "0      0.0   conditional  0.013299   0.955343            0.0     0.041023   \n",
            "1      0.0  filtered_opt  0.027287   0.868159            0.0     0.041023   \n",
            "2     10.0   conditional  0.013565   0.979066            0.0     0.020511   \n",
            "3     10.0  filtered_opt  0.003823   0.991086            0.0     0.020511   \n",
            "4     20.0   conditional  0.028519   0.922022            0.0     0.022528   \n",
            "5     20.0  filtered_opt  0.038024   0.864338            0.0     0.022528   \n",
            "6     30.0   conditional  0.020868   0.947165            0.0     0.001483   \n",
            "7     30.0  filtered_opt  0.028879   0.900559            0.0     0.001483   \n",
            "8     40.0   conditional  0.014222   0.947457            0.0     0.004000   \n",
            "9     40.0  filtered_opt  0.016030   0.933892            0.0     0.004000   \n",
            "10    60.0   conditional  0.020936   0.944056            0.0     0.012000   \n",
            "11    60.0  filtered_opt  0.026877   0.894253            0.0     0.012000   \n",
            "\n",
            "    pearson_r_linear  \n",
            "0           0.869412  \n",
            "1           0.869412  \n",
            "2           0.956306  \n",
            "3           0.956306  \n",
            "4           0.957343  \n",
            "5           0.957343  \n",
            "6           0.998602  \n",
            "7           0.998602  \n",
            "8           0.992794  \n",
            "9           0.992794  \n",
            "10          0.946075  \n",
            "11          0.946075  \n",
            "Generated specs shape: (64, 601)\n",
            "Generated specs_opt shape: (64, 601)\n",
            "Total variation across c midpoints: 9.065722465515137\n"
          ]
        }
      ]
    }
  ]
}